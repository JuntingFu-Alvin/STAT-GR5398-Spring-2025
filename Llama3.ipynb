{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpaIHyLXFOFz"
      },
      "outputs": [],
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Install required packages\\n\",\n",
        "    \"!pip install --upgrade transformers torch peft huggingface-hub\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Log in to Hugging Face (run this cell and paste your token when prompted)\\n\",\n",
        "    \"!huggingface-cli login\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Import libraries\\n\",\n",
        "    \"import os\\n\",\n",
        "    \"import torch\\n\",\n",
        "    \"from transformers import AutoTokenizer, AutoModelForCausalLM\\n\",\n",
        "    \"from peft import PeftModel, LoraConfig\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Define cache directory\\n\",\n",
        "    \"cache_dir = \\\"./pretrained-models\\\"\\n\",\n",
        "    \"os.makedirs(cache_dir, exist_ok=True)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Load Llama-3.1-8B base model\\n\",\n",
        "    \"llama3_base = AutoModelForCausalLM.from_pretrained(\\n\",\n",
        "    \"    \\\"meta-llama/Llama-3.1-8B\\\",\\n\",\n",
        "    \"    trust_remote_code=True,\\n\",\n",
        "    \"    device_map=\\\"auto\\\",\\n\",\n",
        "    \"    cache_dir=cache_dir,\\n\",\n",
        "    \"    torch_dtype=torch.float16,\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Load tokenizer\\n\",\n",
        "    \"llama3_tokenizer = AutoTokenizer.from_pretrained(\\\"meta-llama/Llama-3.1-8B\\\", cache_dir=cache_dir)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# LoRA Fine-Tuning for Llama-3.1-8B\\n\",\n",
        "    \"lora_config = LoraConfig(\\n\",\n",
        "    \"    r=8,\\n\",\n",
        "    \"    lora_alpha=16,\\n\",\n",
        "    \"    lora_dropout=0.1,\\n\",\n",
        "    \"    target_modules=[\\\"q_proj\\\", \\\"v_proj\\\"],\\n\",\n",
        "    \"    task_type=\\\"CAUSAL_LM\\\",\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"llama3_model = get_peft_model(llama3_base, lora_config)\\n\",\n",
        "    \"llama3_model.load_adapter(\\n\",\n",
        "    \"    \\\"./finetuned_models/dow30-202305-202405-llama-3.1-8b_202602280001\\\",  # Placeholder path\\n\",\n",
        "    \"    cache_dir=cache_dir,\\n\",\n",
        "    \")\\n\",\n",
        "    \"llama3_model = llama3_model.eval()\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Testing function for Llama-3.1-8B\\n\",\n",
        "    \"def test_demo(model, tokenizer, prompt):\\n\",\n",
        "    \"    inputs = tokenizer(prompt, return_tensors=\\\"pt\\\", padding=False, max_length=8000)\\n\",\n",
        "    \"    inputs = {key: value.to(model.device) for key, value in inputs.items()}\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    start_time = time.time()\\n\",\n",
        "    \"    outputs = model.generate(\\n\",\n",
        "    \"        **inputs,\\n\",\n",
        "    \"        max_length=4096,\\n\",\n",
        "    \"        do_sample=True,\\n\",\n",
        "    \"        eos_token_id=tokenizer.eos_token_id,\\n\",\n",
        "    \"        use_cache=True,\\n\",\n",
        "    \"    )\\n\",\n",
        "    \"    end_time = time.time()\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\",\n",
        "    \"    return output, end_time - start_time\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Example usage (replace with your dataset)\\n\",\n",
        "    \"prompt = \\\"You are a seasoned stock market analyst. Your task is to list the positive developments and potential concerns for companies based on relevant news and financials from the past weeks, then provide an analysis and prediction for the companies' stock price movement for the upcoming week. Ticker: AXP\\\"\\n\",\n",
        "    \"output, response_time = test_demo(llama3_model, llama3_tokenizer, prompt)\\n\",\n",
        "    \"print(f\\\"Prediction: {output}\\\")\\n\",\n",
        "    \"print(f\\\"Response Time: {response_time:.4f} seconds\\\")\\n\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"language_info\": {\n",
        "   \"name\": \"python\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 2\n",
        "}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "me2G9muTFeGg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}