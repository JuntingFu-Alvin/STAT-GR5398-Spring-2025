{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpaIHyLXFOFz"
      },
      "outputs": [],
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Install required packages\\n\",\n",
        "    \"!pip install --upgrade transformers datasets torch peft sklearn tqdm huggingface-hub\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Log in to Hugging Face (run this cell and paste your token when prompted)\\n\",\n",
        "    \"!huggingface-cli login\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Import libraries\\n\",\n",
        "    \"import os\\n\",\n",
        "    \"import torch\\n\",\n",
        "    \"from transformers import AutoTokenizer, AutoModelForCausalLM\\n\",\n",
        "    \"from datasets import load_dataset\\n\",\n",
        "    \"from sklearn.metrics import accuracy_score\\n\",\n",
        "    \"import tqdm\\n\",\n",
        "    \"from peft import PeftModel\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Define cache directory for pre-trained models\\n\",\n",
        "    \"cache_dir = \\\"./pretrained-models\\\"\\n\",\n",
        "    \"os.makedirs(cache_dir, exist_ok=True)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Load the FinGPT-Forecaster dataset (Dow 30 dataset)\\n\",\n",
        "    \"dataset = load_dataset(\\\"FinGPT/fingpt-forecaster-dow30-202305-202405\\\", split=\\\"train\\\")\\n\",\n",
        "    \"print(f\\\"Dataset loaded with {len(dataset)} rows\\\")\\n\",\n",
        "    \"print(dataset.column_names)  # Should show 'prompt', 'answer', 'period', 'label', 'symbol'\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Load Llama3-1-8B base model (authenticated via huggingface-cli login)\\n\",\n",
        "    \"llama3_base = AutoModelForCausalLM.from_pretrained(\\n\",\n",
        "    \"    \\\"meta-llama/Llama-3.1-8B\\\",\\n\",\n",
        "    \"    trust_remote_code=True,\\n\",\n",
        "    \"    device_map=\\\"auto\\\",\\n\",\n",
        "    \"    cache_dir=cache_dir,\\n\",\n",
        "    \"    torch_dtype=torch.float16,\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Load DeepSeek base model (assuming it’s accessible)\\n\",\n",
        "    \"deepseek_base = AutoModelForCausalLM.from_pretrained(\\n\",\n",
        "    \"    \\\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\\\",\\n\",\n",
        "    \"    trust_remote_code=True,\\n\",\n",
        "    \"    device_map=\\\"auto\\\",\\n\",\n",
        "    \"    cache_dir=cache_dir,\\n\",\n",
        "    \"    torch_dtype=torch.float16,\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Load tokenizers for both models\\n\",\n",
        "    \"llama3_tokenizer = AutoTokenizer.from_pretrained(\\\"meta-llama/Llama-3.1-8B\\\", cache_dir=cache_dir)\\n\",\n",
        "    \"deepseek_tokenizer = AutoTokenizer.from_pretrained(\\\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\\\", cache_dir=cache_dir)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Data Preprocessing and Filtering\\n\",\n",
        "    \"def filter_by_ticker(test_dataset, ticker_code):\\n\",\n",
        "    \"    filtered_data = []\\n\",\n",
        "    \"    for row in test_dataset:\\n\",\n",
        "    \"        prompt_content = row['prompt']\\n\",\n",
        "    \"        ticker_symbol = re.search(r\\\"ticker['s]?[(A-Z)+]\\\", prompt_content)\\n\",\n",
        "    \"        if ticker_symbol and ticker_symbol.group(1) == ticker_code:\\n\",\n",
        "    \"            filtered_data.append(row)\\n\",\n",
        "    \"    filtered_dataset = Dataset.from_dict({key: [row[key] for row in filtered_data] for key in test_dataset.column_names})\\n\",\n",
        "    \"    return filtered_dataset\\n\",\n",
        "    \"\\n\",\n",
        "    \"def get_unique_ticker_symbols(test_dataset):\\n\",\n",
        "    \"    ticker_symbols = set()\\n\",\n",
        "    \"    for i in range(len(test_dataset)):\\n\",\n",
        "    \"        prompt_content = test_dataset[i]['prompt']\\n\",\n",
        "    \"        ticker_symbol = re.search(r\\\"ticker['s]?[(A-Z)+]\\\", prompt_content)\\n\",\n",
        "    \"        if ticker_symbol:\\n\",\n",
        "    \"            ticker_symbols.add(ticker_symbol.group(1))\\n\",\n",
        "    \"    return list(ticker_symbols)\\n\",\n",
        "    \"\\n\",\n",
        "    \"def insert_guidance_after_intro(prompt):\\n\",\n",
        "    \"    intro_marker = \\\"You are a seasoned stock market analyst. Your task is to list the positive developments and \\\"\\n\",\n",
        "    \"    guidance = \\\"potential concerns for companies based on relevant news and financials from the past weeks, then provide an analysis and prediction for the companies' stock price movement for the upcoming week.\\\"\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    intro_pos = prompt.find(intro_marker)\\n\",\n",
        "    \"    guidance_start_pos = prompt.find(guidance, intro_pos)\\n\",\n",
        "    \"    guidance_end_pos = guidance_start_pos + len(guidance) if guidance_start_pos != -1 else -1\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    if intro_pos == -1 or guidance_start_pos == -1 or guidance_end_pos == -1:\\n\",\n",
        "    \"        return prompt\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    guidance_section = prompt[guidance_start_pos:guidance_end_pos].strip()\\n\",\n",
        "    \"    new_prompt = (\\n\",\n",
        "    \"        prompt[:intro_pos + len(intro_marker)] + \\\"\\\\n\\\" +\\n\",\n",
        "    \"        guidance + \\\"\\\\n\\\" +\\n\",\n",
        "    \"        prompt[guidance_end_pos:]\\n\",\n",
        "    \"    )\\n\",\n",
        "    \"    return new_prompt\\n\",\n",
        "    \"\\n\",\n",
        "    \"import re\\n\",\n",
        "    \"from datasets import Dataset\\n\",\n",
        "    \"\\n\",\n",
        "    \"test_dataset = dataset.filter(lambda x: x[\\\"prompt\\\"], input_columns=[\\\"prompt\\\"])\\n\",\n",
        "    \"test_dataset = test_dataset.map(lambda x: {\\\"prompt\\\": insert_guidance_after_intro(x[\\\"prompt\\\"])})\\n\",\n",
        "    \"filtered_dataset = filter_by_ticker(test_dataset, \\\"AXP\\\")\\n\",\n",
        "    \"print(f\\\"Filtered dataset for AXP has {len(filtered_dataset)} rows\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# LoRA Fine-Tuning for Both Models\\n\",\n",
        "    \"from peft import LoraConfig, get_peft_model\\n\",\n",
        "    \"\\n\",\n",
        "    \"lora_config = LoraConfig(\\n\",\n",
        "    \"    r=8,  # Rank of the adaptation\\n\",\n",
        "    \"    lora_alpha=16,\\n\",\n",
        "    \"    lora_dropout=0.1,\\n\",\n",
        "    \"    target_modules=[\\\"q_proj\\\", \\\"v_proj\\\"],  # Adjust based on Llama’s architecture\\n\",\n",
        "    \"    task_type=\\\"CAUSAL_LM\\\",\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Fine-tune Llama-3.1-8B\\n\",\n",
        "    \"llama3_model = get_peft_model(llama3_base, lora_config)\\n\",\n",
        "    \"llama3_model.load_adapter(\\n\",\n",
        "    \"    \\\"./finetuned_models/dow30-202305-202405-llama-3.1-8b_202602280001\\\",  # Placeholder path\\n\",\n",
        "    \"    cache_dir=cache_dir,\\n\",\n",
        "    \")\\n\",\n",
        "    \"llama3_model = llama3_model.eval()\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Fine-tune DeepSeek-R1-Distill-Llama-8B\\n\",\n",
        "    \"deepseek_model = get_peft_model(deepseek_base, lora_config)\\n\",\n",
        "    \"deepseek_model.load_adapter(\\n\",\n",
        "    \"    \\\"./finetuned_models/dow30-202305-202405-DeepSeek-R1-Distill-llama-8B_202592020557\\\",\\n\",\n",
        "    \"    cache_dir=cache_dir,\\n\",\n",
        "    \")\\n\",\n",
        "    \"deepseek_model = deepseek_model.eval()\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Testing and Evaluation Function\\n\",\n",
        "    \"def test_demo(model, tokenizer, prompt):\\n\",\n",
        "    \"    inputs = tokenizer(prompt, return_tensors=\\\"pt\\\", padding=False, max_length=8000)\\n\",\n",
        "    \"    inputs = {key: value.to(model.device) for key, value in inputs.items()}\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    start_time = time.time()\\n\",\n",
        "    \"    outputs = model.generate(\\n\",\n",
        "    \"        **inputs,\\n\",\n",
        "    \"        max_length=4096,\\n\",\n",
        "    \"        do_sample=True,\\n\",\n",
        "    \"        eos_token_id=tokenizer.eos_token_id,\\n\",\n",
        "    \"        use_cache=True,\\n\",\n",
        "    \"    )\\n\",\n",
        "    \"    end_time = time.time()\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\",\n",
        "    \"    return output, end_time - start_time\\n\",\n",
        "    \"\\n\",\n",
        "    \"def test_acc(test_dataset, model_name):\\n\",\n",
        "    \"    answers_base, answers_fine_tuned, gts, times_base, times_fine_tuned = [], [], [], [], []\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    if model_name == \\\"llama3\\\":\\n\",\n",
        "    \"        base_model = llama3_base\\n\",\n",
        "    \"        fine_tuned_model = llama3_model\\n\",\n",
        "    \"        tokenizer = llama3_tokenizer\\n\",\n",
        "    \"    elif model_name == \\\"deepseek\\\":\\n\",\n",
        "    \"        base_model = deepseek_base\\n\",\n",
        "    \"        fine_tuned_model = deepseek_model\\n\",\n",
        "    \"        tokenizer = deepseek_tokenizer\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    for i in tqdm(range(len(test_dataset)), desc=\\\"Processing test samples\\\"):\\n\",\n",
        "    \"        try:\\n\",\n",
        "    \"            prompt = test_dataset[i][\\\"prompt\\\"]\\n\",\n",
        "    \"            gt = test_dataset[i][\\\"answer\\\"]\\n\",\n",
        "    \"            \\n\",\n",
        "    \"            output_base, time_base = test_demo(base_model, tokenizer, prompt)\\n\",\n",
        "    \"            answer_base = re.sub(r\\\"\\\\[INST\\\\]\\\\s*|\\\\s*\\\\[/INST\\\\]\\\", \\\"\\\", output_base, flags=re.DOTALL)\\n\",\n",
        "    \"            \\n\",\n",
        "    \"            output_fine_tuned, time_fine_tuned = test_demo(fine_tuned_model, tokenizer, prompt)\\n\",\n",
        "    \"            answer_fine_tuned = re.sub(r\\\"\\\\[INST\\\\]\\\\s*|\\\\s*\\\\[/INST\\\\]\\\", \\\"\\\", output_fine_tuned, flags=re.DOTALL)\\n\",\n",
        "    \"            \\n\",\n",
        "    \"            answers_base.append(answer_base)\\n\",\n",
        "    \"            answers_fine_tuned.append(answer_fine_tuned)\\n\",\n",
        "    \"            gts.append(gt)\\n\",\n",
        "    \"            times_base.append(time_base)\\n\",\n",
        "    \"            times_fine_tuned.append(time_fine_tuned)\\n\",\n",
        "    \"        except Exception as e:\\n\",\n",
        "    \"            print(f\\\"Error processing sample {i}: {e}\\\")\\n\",\n",
        "    \"            return answers_base, answers_fine_tuned, gts, times_base, times_fine_tuned\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    return answers_base, answers_fine_tuned, gts, times_base, times_fine_tuned\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Test both models\\n\",\n",
        "    \"llama3_answers_base, llama3_answers_fine_tuned, llama3_gts, llama3_base_times, llama3_fine_tuned_times = test_acc(filtered_dataset, \\\"llama3\\\")\\n\",\n",
        "    \"deepseek_answers_base, deepseek_answers_fine_tuned, deepseek_gts, deepseek_base_times, deepseek_fine_tuned_times = test_acc(filtered_dataset, \\\"deepseek\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Metrics Calculation and Comparison\\n\",\n",
        "    \"def calc_metrics(predictions, ground_truths):\\n\",\n",
        "    \"    predictions_clean = [pred.strip().lower() for pred in predictions]\\n\",\n",
        "    \"    ground_truths_clean = [gt.strip().lower() for gt in ground_truths]\\n\",\n",
        "    \"    accuracy = accuracy_score(ground_truths_clean, predictions_clean)\\n\",\n",
        "    \"    return {\\\"accuracy\\\": accuracy}\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Calculate metrics for Llama3\\n\",\n",
        "    \"llama3_base_metrics = calc_metrics(llama3_answers_base, llama3_gts)\\n\",\n",
        "    \"llama3_fine_tuned_metrics = calc_metrics(llama3_answers_fine_tuned, llama3_gts)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Calculate metrics for DeepSeek\\n\",\n",
        "    \"deepseek_base_metrics = calc_metrics(deepseek_answers_base, deepseek_gts)\\n\",\n",
        "    \"deepseek_fine_tuned_metrics = calc_metrics(deepseek_answers_fine_tuned, deepseek_gts)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Compare metrics\\n\",\n",
        "    \"comparing_metrics = calc_metrics(llama3_answers_fine_tuned, deepseek_answers_fine_tuned)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Print results\\n\",\n",
        "    \"print(\\\"Evaluating Base Model...\\\")\\n\",\n",
        "    \"print(\\\"Llama3 Base Metrics:\\\", llama3_base_metrics)\\n\",\n",
        "    \"print(\\\"DeepSeek Base Metrics:\\\", deepseek_base_metrics)\\n\",\n",
        "    \"print(\\\"\\\\nEvaluating Fine-Tuned Model...\\\")\\n\",\n",
        "    \"print(\\\"Llama3 Fine-Tuned Metrics:\\\", llama3_fine_tuned_metrics)\\n\",\n",
        "    \"print(\\\"DeepSeek Fine-Tuned Metrics:\\\", deepseek_fine_tuned_metrics)\\n\",\n",
        "    \"print(\\\"\\\\nComparing Fine-Tuned Models:\\\")\\n\",\n",
        "    \"print(\\\"Comparison Metrics:\\\", comparing_metrics)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Calculate average response times\\n\",\n",
        "    \"llama3_base_avg_time = sum(llama3_base_times) / len(llama3_base_times) if llama3_base_times else 0\\n\",\n",
        "    \"llama3_fine_tuned_avg_time = sum(llama3_fine_tuned_times) / len(llama3_fine_tuned_times) if llama3_fine_tuned_times else 0\\n\",\n",
        "    \"deepseek_base_avg_time = sum(deepseek_base_times) / len(deepseek_base_times) if deepseek_base_times else 0\\n\",\n",
        "    \"deepseek_fine_tuned_avg_time = sum(deepseek_fine_tuned_times) / len(deepseek_fine_tuned_times) if deepseek_fine_tuned_times else 0\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\nAverage Response Times (seconds):\\\")\\n\",\n",
        "    \"print(f\\\"Llama3 Base: {llama3_base_avg_time:.4f}\\\")\\n\",\n",
        "    \"print(f\\\"Llama3 Fine-Tuned: {llama3_fine_tuned_avg_time:.4f}\\\")\\n\",\n",
        "    \"print(f\\\"DeepSeek Base: {deepseek_base_avg_time:.4f}\\\")\\n\",\n",
        "    \"print(f\\\"DeepSeek Fine-Tuned: {deepseek_fine_tuned_avg_time:.4f}\\\")\\n\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"language_info\": {\n",
        "   \"name\": \"python\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 2\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "me2G9muTFeGg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}