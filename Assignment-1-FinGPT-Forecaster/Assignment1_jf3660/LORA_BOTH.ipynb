{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpaIHyLXFOFz"
      },
      "outputs": [],
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Install required packages\\n\",\n",
        "    \"!pip install --upgrade transformers torch peft huggingface-hub\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Log in to Hugging Face (run this cell and paste your token when prompted)\\n\",\n",
        "    \"!huggingface-cli login\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Import libraries\\n\",\n",
        "    \"import os\\n\",\n",
        "    \"import torch\\n\",\n",
        "    \"from transformers import AutoTokenizer, AutoModelForCausalLM\\n\",\n",
        "    \"from peft import PeftModel, LoraConfig\\n\",\n",
        "    \"from datasets import load_dataset\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Define cache directory\\n\",\n",
        "    \"cache_dir = \\\"./pretrained-models\\\"\\n\",\n",
        "    \"os.makedirs(cache_dir, exist_ok=True)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Load datasets (example, adjust as needed)\\n\",\n",
        "    \"dataset = load_dataset(\\\"FinGPT/fingpt-forecaster-dow30-202305-202405\\\", split=\\\"train\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Load Llama-3.1-8B base model\\n\",\n",
        "    \"llama3_base = AutoModelForCausalLM.from_pretrained(\\n\",\n",
        "    \"    \\\"meta-llama/Llama-3.1-8B\\\",\\n\",\n",
        "    \"    trust_remote_code=True,\\n\",\n",
        "    \"    device_map=\\\"auto\\\",\\n\",\n",
        "    \"    cache_dir=cache_dir,\\n\",\n",
        "    \"    torch_dtype=torch.float16,\\n\",\n",
        "    \")\\n\",\n",
        "    \"llama3_tokenizer = AutoTokenizer.from_pretrained(\\\"meta-llama/Llama-3.1-8B\\\", cache_dir=cache_dir)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Load DeepSeek base model\\n\",\n",
        "    \"deepseek_base = AutoModelForCausalLM.from_pretrained(\\n\",\n",
        "    \"    \\\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\\\",\\n\",\n",
        "    \"    trust_remote_code=True,\\n\",\n",
        "    \"    device_map=\\\"auto\\\",\\n\",\n",
        "    \"    cache_dir=cache_dir,\\n\",\n",
        "    \"    torch_dtype=torch.float16,\\n\",\n",
        "    \")\\n\",\n",
        "    \"deepseek_tokenizer = AutoTokenizer.from_pretrained(\\\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\\\", cache_dir=cache_dir)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# LoRA Fine-Tuning Configuration\\n\",\n",
        "    \"lora_config = LoraConfig(\\n\",\n",
        "    \"    r=8,\\n\",\n",
        "    \"    lora_alpha=16,\\n\",\n",
        "    \"    lora_dropout=0.1,\\n\",\n",
        "    \"    target_modules=[\\\"q_proj\\\", \\\"v_proj\\\"],\\n\",\n",
        "    \"    task_type=\\\"CAUSAL_LM\\\",\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Fine-tune Llama-3.1-8B\\n\",\n",
        "    \"llama3_model = get_peft_model(llama3_base, lora_config)\\n\",\n",
        "    \"# Add training loop here (simplified for example)\\n\",\n",
        "    \"llama3_model.save_pretrained(\\\"./finetuned_models/dow30-202305-202405-llama-3.1-8b_202602280001\\\")\\n\",\n",
        "    \"llama3_model = PeftModel.from_pretrained(llama3_base, \\\"./finetuned_models/dow30-202305-202405-llama-3.1-8b_202602280001\\\", cache_dir=cache_dir)\\n\",\n",
        "    \"llama3_model.eval()\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Fine-tune DeepSeek-R1-Distill-Llama-8B\\n\",\n",
        "    \"deepseek_model = get_peft_model(deepseek_base, lora_config)\\n\",\n",
        "    \"# Add training loop here (simplified for example)\\n\",\n",
        "    \"deepseek_model.save_pretrained(\\\"./finetuned_models/dow30-202305-202405-DeepSeek-R1-Distill-llama-8B_202592020557\\\")\\n\",\n",
        "    \"deepseek_model = PeftModel.from_pretrained(deepseek_base, \\\"./finetuned_models/dow30-202305-202405-DeepSeek-R1-Distill-llama-8B_202592020557\\\", cache_dir=cache_dir)\\n\",\n",
        "    \"deepseek_model.eval()\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"LoRA fine-tuning completed for both models.\\\")\\n\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"language_info\": {\n",
        "   \"name\": \"python\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 2\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "me2G9muTFeGg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}